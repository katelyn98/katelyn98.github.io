<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Katelyn C. Morrison


  | Exploring XRAI Saliency Map

</title>
<meta name="description" content="Katelyn Morrison's personal website. Learn more about her latest research projects and more!
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒŒ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/old_posts/2021-11-20-xrai-explore/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-XXXXXXXXX');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>
    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://katelyn98.github.io//">
       Katelyn C. Morrison
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/travel/">
                travel
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Exploring XRAI Saliency Map</h1>
    <p class="post-description">Evaluating XRAI Saliency Map under different scenarios using different models.</p>
  </header>

  <article>
    <h2 id="tutorial-detailsnotes">Tutorial Details/Notes</h2>

<p>Followed a Google PAIR tutorial on their saliency maps (including XRAI). The tutorial can be found <a href="https://github.com/PAIR-code/saliency/blob/master/Examples_pytorch.ipynb" target="_blank" rel="noopener noreferrer">here</a>. The tutorial is based on pytorch (what I am familiar with using). The tutorial is using a pretrained inception_v3 model from pytorch.</p>

<h2 id="brief-introduction-to-xrai">Brief introduction to XRAI</h2>

<p>View <a href="https://arxiv.org/pdf/1906.02825.pdf" target="_blank" rel="noopener noreferrer">this paper</a> and <a href="#">this code repository</a> to learn more about XRAI from Google PAIR team. Something unique about this method is that it can focus on multiple regions in an image.</p>

<h2 id="experiments-and-outcomes">Experiments and Outcomes</h2>

<p>More experiments that I did can be found in this <a href="https://drive.google.com/file/d/1JcLeXmbA68CO0hANTVFVv-jGBa61Kvk-/view?usp=sharing" target="_blank" rel="noopener noreferrer"><strong>Google Colab</strong></a> which is kinda messy at the moment. I think that code readability, reproducability, and documentation is extremely improtant for reserach to excel.</p>

<p><em>Below are the outcomes of the tutorial along with further directions I investigated. These are all preliminary results. I am showcasing the experiments to highlight my ability to use these tools and ask interesting questions.</em></p>

<h3 id="inception-v3-vs-vgg16">Inception V3 vs VGG16</h3>

<p><strong><em>Inception V3</em></strong></p>

<p><img src="/assets/img/inceptionv3top30.png" alt="drawing" width="100%"></p>

<p>The tutorial uses a doberman image as their example. You can see based on the XRAI heatmap that the most important feature was the snout of the doberman to classify it specifically as a doberman. The top 30% is the most salient 30% of the image. It includes some portions of the sidewalk here. Below we can also so the most salient 10% of the image.</p>

<p><img src="/assets/img/inceptionv3.png" alt="drawing" width="100%"></p>

<p><strong><em>VGG-16</em></strong></p>

<p><img src="/assets/img/vgg16.png" alt="drawing" width="100%"></p>

<p>The VGG did predict that this was a doberman, but you can see that it is focusing on different features to get to its conclusion. It really highlights the whole shape of the dog in the XRAI heatmap as well whereas the inceptionv3 did not focus that strongly on shape of the dog, but primarily the snout.</p>

<hr>

<h3 id="human-vs-inception-v3--vgg16">Human vs Inception V3 &amp; VGG16</h3>

<p>After seeing how these two models produced significantly different XRAI heatmaps, I wondered how humans would produce this heatmap for the same image. I conducted a very low-cost study to find some preliminary results. I drafted up some very <a href="https://drive.google.com/file/d/1CuVTnwA4-vSRJcl0fie9-BxVMut4GXo0/view?usp=sharing" target="_blank" rel="noopener noreferrer">simple directions</a> and sent it to four of my friends. Their results can be seen below.</p>

<p><img src="/assets/img/humanresults.png" alt="drawing" width="100%"></p>

<p><strong>Future Work</strong>. Running this study on a large scale and then averaging the color for every pixel from the study results would generate one final image which may or may not look similar to the modelâ€™s XRAI heatmap. If it does look similar, this can suggest that humans do use similar features that these models use when reasoning the classification of an image. The collected human-generated saliency maps can be used to fine tune a model to generate saliency maps that are human-like. It could be useful to just see the output of this expeirment regardless to identify future directions.</p>

<p>(<em>View doberman <a href="/assets/img/dobermanorig.png" alt="doberman" width="100%">image</a> - for testing purposes</em>)</p>

<hr>

<h3 id="out-of-distribution-images">Out-of-distribution Images</h3>

<p><strong>Corruptions</strong></p>

<p>Different convolutional models perform slightly different when presented corrupted images. This can be determined by just reading the prediction from the model. For example, when you apply a glass blur of severity 2 on the image, an inception_v3 will predict it is an Airedale terrier. When you use a VGG it will predict that it is instead a Tibetan mastiff. All you can see is the prediction of each of these two models, but viewing the saliency map for both of these predictions will show how it deviates from the important features in the original image.</p>

<p><img src="/assets/img/inceptionv3glassblur2.png" alt="drawing" width="100%"></p>

<p><em>Corruption glass blur of severity 2 applied. Inception V3 predicted an Airedale terrier. Note: this image was obtained from <a href="https://github.com/hendrycks/robustness" target="_blank" rel="noopener noreferrer">ImageNet-C</a>.</em></p>

<p>The model is looking at different features when the image is slightly corrupt with glass blur (severity level 2). It focuses more on the shape of the dogs body in the XRAI heatmap here, too.</p>

<p><img src="/assets/img/vgg16glassblur2.png" alt="drawing" width="100%"></p>

<p><em>Corruption glass blur of severity 2. VGG-16 predicted a Rottweiler. Note: this image was obtained from <a href="https://github.com/hendrycks/robustness" target="_blank" rel="noopener noreferrer">ImageNet-C</a>.</em></p>

<p>Here, the model is looking again strongly at the shape of the image, but is focusing on different features to get to its decision.</p>

<p><strong>Shape-Texture Bias</strong></p>

<p>To learn more about shape bias and the texture cue-conflict dataset, please view this <a href="https://github.com/rgeirhos/texture-vs-shape" target="_blank" rel="noopener noreferrer">GitHub repository</a>. Looking at conflicting shapes/texture stylization where the shape is a dpg amd the texture is an elephant:</p>

<p><img src="/assets/img/inceptionv3shapebias.png" alt="drawing" width="100%"></p>

<p>Shape of dog stylized with texture of elephant. InceptionV3 prediction is â€˜African elephant, Loxodonta africanaâ€™. We can see from XRAI that this model focuses heavily on the texture markings of the elephant instead of the general shape of the dog.</p>

<p>Now looking at where the shape is a cat and the texture is an elephant:</p>

<p><img src="/assets/img/inceptionv3shapebiascat.png" alt="drawing" width="100%"></p>

<p>The InceptionV3 prediction was also â€˜African elephant, Loxodonta africanaâ€™. We can see it is focusing on the creases in the elephant skin.</p>

<hr>

<h3 id="future-work">Future Work</h3>

<p>These results are just preliminary experiments that I did. More work should be done on other models, other corruptions from ImageNet-C, and other texture cue-conflicts. Should continue this more on other severity levels, other corruptions, and other models.</p>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2025 Katelyn C. Morrison.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: July 30, 2025.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>

---
layout: distill
title: Towards Generating Human-Centered Saliency Maps without Sacrificing Accuracy
date: 2022-05-04
img: /assets/img/placeholderpicture.png
importance: 4
tags: research courses
categories: Explainability AI

authors:
  - name: Katelyn Morrison
    url: "https://cs.cmu.edu/~kcmorris"
    affiliations:
      name: HCII, CMU
  - name: Vivek Aswal
    url: "#"
    affiliations:
      name: ECE, CMU
  - name: Ashley Kim
    url: "#"
    affiliations:
      name: ECE, CMU
  - name: Gore Kao
    url: "#"
    affiliations:
      name: ECE, CMU

bibliography: 2022-05-09-vlr.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    subsections:
      - name: Research Question & Contributions
  - name: Related Work
  - name: Methods
    subsections:
      - name: Empirical Study
      - name: Data Augmentations
      - name: Experiment Design
  - name: Results
    subsections:
      - name: Empirical Study
      - name: Main Experiment 
  - name: Limtations
  - name: Future Work & Conclusion
  - name: Code

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---



## Introduction

Deep neural networks have made significant advances in wide-ranging tasks including image classification, segmentation, in-painting, captioning, object detection, action recognition, deep fakes, etc. However, deep neural networks often lack reliability in the real world because they are "black-boxes" and can depend on spurious input features that are not essential to the true label. 

Just include the motivation and the problem in the introduction.

### Research Questions & Contributions

Therefore, our primary research questions are:
* [**RQ1**] How do current state-of-the-art object detection models compare to human attention? 
  - [**H1**] We hypothesize that current state-of-the-art object detection models do not nearly compare to human attention.
* [**RQ2**] What techniques will cause state-of-the-art object detection models to generate saliency maps that are similar to human eye-fixations while maintaining accuracy?
  - [**H2**] We hypothesize that some form of data augmentation that penalizes spurious patterns will result in more human-centered saliency maps.

Our primary contributions include: 
* We conduct a small empirical study to understand how seven current object detection models compare to human attention.
* We present a novel data augmentation technique called *Selective Inpainting* that can be used for augmenting images for image classification and object detection models
* We evaluate the impact that different data augmentation techniques have on saliency maps generated by Faster R-CNN.

## Related Works

Add some related works sections here about human attention compared to deep learning and recent approaches people have done to improve deep learning interpretability based on human attention


## Methods

Brief introduction about what we did

### Empirical Study

We conducted an empirical study to gain an understanding of which state-of-the-art object detection models currently generate saliency maps similar to human attention. We evaluated and compared saliency maps generated by seven different object detection models available on PyTorch to human attention maps and predicted human eye-fixations. To obtain the human attention maps, we used the human attention maps for PASCAL2012 from the [ML-Interpretability-Evaluation-Benchmark](https://github.com/SinaMohseni/ML-Interpretability-Evaluation-Benchmark) <d-cite key="mohseni2020benchmark"></d-cite>. To obtain predicted human eye-fixations, we used the DeepGazeIIE saliency prediction model <d-cite key="DBLP:journals/corr/abs-2105-12441"></d-cite>. The specific state-of-the-art object detection models we evaluated and compared to the human attention maps and predicted eye-fixations include YOLOV5, Faster R-CNN with a ResNet-50 backbone, SSD with a VGG backbone, SSD with a MobileNet background, Mask R-CNN, RetinaNet, and DETR. 

**Experiment Details**

We generated a saliency map for every single image that had an associated human attention map (ground truth saliency) from the ML-Interpretability-Evaluation-Benchmark. Each image was resized to $$
512$$ x $$512
$$ 
before being evaluated on by the model. The saliency map for the object detection model is generated using the EigenCAM method (<d-cite key="DBLP:journals/corr/abs-2008-00299"></d-cite>) from the PyTorch library for CAM methods <d-cite key="jacobgilpytorchcam"></d-cite>. Once the saliency map from the object detection model is generated, the mean absolute error (MAE) is calculated between the generated saliency map and the human attention map. The MAE is also calculated between the generated saliency map and the predicted human eye-fixations (produced from the DeepGazeIIE model). While the MAE to some extent can reveal how similar the saliency maps are, we also calculate intersection over union (IoU) between the top $$90\%$$ salient pixels of the generated saliency map and the top $$90\%$$ salient pixels from the human attention map/predicted human eye-fixation. Calculating the IoU will help reveal if the most salient region identify by the model and the humans align <d-cite key="DBLP:journals/corr/abs-2107-09234"></d-cite>.


### Data Augmentations

Data augmentation for object detection is slightly more complex than data augmentation for image classification tasks because of the associated bounding boxes for each object <d-cite key="DBLP:journals/corr/abs-1906-11172"></d-cite>. 

We use three different data augmentation techniques: *selective erasing*, *selective inpainting*, and *non-trivial transformations*. Below, we define and provide examples of each of these data augmentations. 

**Selective Erasing**

The goal of selective erasing is to get rid of potential spurious patterns, patterns that the model has learned to associate with a label even though it does not represent that label [*CITATION*]. In order to augment images using selective erasing, we send the image through Faster R-CNN and use EigenCAM to generate the saliency map. We then send the image through the DeepGazeIIE model to generated the predicted eye-fixations map. We calculate the mean absolute error (MAE) between the two saliency maps. If the MAE is below 0.1, meaning the two saliency maps are significantly different from one another, then we erase the top 2.5% salient pixels identified from the Faster R-CNN saliency map from the original image. An example of this process and the outcomes from each step are shown in Figure 1

<figure>
<img src="/assets/img/selective_augment.png" alt="dataset augmentations" width="100%"/>
<figcaption>Figure 1: Example of how the selective erasing and selective inpainting augmentation techniques work. If the MAE between the Faster R-CNN saliency map and DeepGaze saliency map is less than 0.1 than erase the top 2.5% salient pixels from the original image. Using an untrained neural network, inpaint the pixels that were erased.</figcaption>
</figure>


**Selective Inpainting**

The selective inpainting augmentation process follows the same steps as selective erasing and then inpaints the erased image. To inpaint the top 2.5% salient pixels as denoted by Faster R-CNN, we send the selective erased image and mask into an untrained neural network and optimize on learning the pixels that minimize the chosen loss function. [ADd details here about the Deep Image Prior Paper <d-cite key="DBLP:journals/corr/abs-1711-10925"></d-cite>.]


**Non-trivial Transformations**

<figure>
<img src="/assets/img/non-trivial_firstpart.png" alt="dataset augmentations non-trivial" width="100%"/>
<figcaption>Figure 2: Sample of different augmentations used in the non-trivial transformations function. Augmentations that are a part of the PIL library (cite or link) or bbaug (need to cite or link) library were considered. More non-trivial augmentation examples can be found in the footnotes <d-footnote>
  <figure>
  <img src="/assets/img/non-trivial_secondpart.png" alt="dataset augmentations non-trivial" width="100%"/>
  <img src="/assets/img/non-trivial_thirdpart.png" alt="dataset augmentations non-trivial" width="100%"/>
  <figcaption>More examples of augmentations considered in the non-trivial transformations.</figcaption>
  </figure>
</d-footnote>.</figcaption>
</figure>


### Experiment Design

Describe how we designed the final experiment we did between faster r-cnn with a resnet 50 backbone. Make sure to add in specifics about fine-tuning etc. 

## Results

Describe our results here. Shows charts, tables, pictures, etc.

### Empirical Study

Results for comparing the saliency maps generated by object detection models to the DeepGazeIIE's predicted eye-fixations are ....
The MAE (mean absolute error) is preferred to be close to 0.0; the IoU (intersection over union) is preferred to be close to 1.0. 

**Table 1: Baseline Performance Metrics for Object Detection Models on Test Data**

| Model       | mAP        |
| :--:       |    :--:   | 
| YOLOV5      | $00000$      |
| SSD-VGG16   | $0000$       | 
| SSD-MobileNet   | $0000$        | 
| Faster R-CNN   | $00000$    | 
| RetinaNet   | $000000$   | 
| Mask R-CNN   | $000000$       | 
| DETR   | $000000$      | 


**Table 2: Object Detection Models compared to DeepGazeIIE Predicted Eye-Fixations for MIT1003**

| Model       | MAE         | IoU           |
| :--:       |    :--:   |     :--:     |
| YOLOV5      | $0.1799$      | $0.1895$   |
| SSD-VGG16   | $0.1643$       | **0.2379**     |
| SSD-MobileNet   | $0.1553$        | $0.1670$     |
| Faster R-CNN   | $0.1441$     | $0.1896$      |
| RetinaNet   | $0.2966$   | $0.1857$    |
| Mask R-CNN   | $0.1550$       | $0.1678$    |
| DETR   | **0.1275**     | $0.2136$     |


**Table 3: Object Detection Models compared to DeepGazeIIE Predicted Eye-Fixations for PASCALVOC2012**

| Model       | MAE         | IoU           |
| :--:       |    :--:   |     :--:     |
| YOLOV5      | $0.2147$      | $0.1837$   |
| SSD-VGG16   | $0.1731$       | **0.2474**     |
| SSD-MobileNet   | $0.1737$        | $0.2086$     |
| Faster R-CNN   | **0.1700**     | $0.2382$      |
| RetinaNet   | $0.2578$   | $0.2158$    |
| Mask R-CNN   | $0.1753$       | $0.2353$    |
| DETR   | $0.1913$      | $0.1664$     |

**Table 4: Object Detection Models compared to Human Attention Masks for PASCALVOC2012**

| Model       | MAE         | IoU           |
| :--:       |    :--:   |     :--:     |
| YOLOV5      | $0.1571$      | $0.2400$   |
| SSD-VGG16   | $0.1277$       | **0.3225**     |
| SSD-MobileNet   | $0.1765$        | $0.2086$     |
| Faster R-CNN   | **0.1145**     | $0.2438$      |
| RetinaNet   | $0.2073$   | $0.2313$    |
| Mask R-CNN   | $0.1254$       | $0.2234$    |
| DETR   | $0.1519$      | $0.2100$     |

### Main Experiment

Show the results from the fine-tuning or training the models here. 


| Augmentation      | mAP        | MAE | IoU |
| :--:       |    :--:   |     :--:   |     :--:   | 
| Selective Erasing      | $00000$      |  $00000$    |   $00000$    |  
| Selective Inpainting   | $0000$       | $0000$       |  $0000$       | 
| Non-Trivial Transformations   | $0000$        | $0000$       | $0000$       | 

## Limitations

**Access to Powerful Computers**

We did this entire project on Google Colab which limited us in terms of the GPU that we could use and the amount of memory we had. We were able to use Colab Pro, but even then the GPUs we were using were limited to 16GB which forced us to use smaller batch sizes than normal. 

**Another limitation**

Some details about that limitation.

## Future Work & Conclusion

Future work should explore these questions for image classification instead of object detection. 

## Code

<img src="/assets/img/github-brands.svg" alt="logo" width="3%"/> [GitHub Repository](https://github.com/Gkao03/Saliency-Map-Visualization)


<img src="/assets/img/google-drive-brands.svg" alt="logo" width="3%"/> [Augmented Models](https://drive.google.com/drive/folders/1BW068VgVoj0_CVFCGED3E61sekw-1R74?usp=sharing)


<!-- ## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

***

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

**Note:** `<d-code>` blocks do not look well in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:

{% highlight javascript %}
var x = 25;
function(x) {
  return x * x;
}
{% endhighlight %}

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

***

## Other Typography?

Emphasis, aka italics, with *asterisks* (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or __underscores__.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
⋅⋅* Unordered sub-list. 
1. Actual numbers don't matter, just that it's a number
⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

* Unordered list can use asterisks
- Or minuses
+ Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[I'm a relative reference to a repository file](../blob/master/LICENSE)

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or <http://www.example.com> and sometimes 
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style: 
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style: 
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 2"

Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```
 
```python
s = "Python syntax highlighting"
print s
```
 
```
No language indicated, so no syntax highlighting. 
But let's throw in a <b>tag</b>.
```

Colons can be used to align columns.

| Tables        | Are           | Cool  |
| ------------- |:-------------:| -----:|
| col 3 is      | right-aligned | $1600 |
| col 2 is      | centered      |   $12 |
| zebra stripes | are neat      |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the 
raw Markdown line up prettily. You can also use inline Markdown.

Markdown | Less | Pretty
--- | --- | ---
*Still* | `renders` | **nicely**
1 | 2 | 3

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. 


Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a *separate paragraph*.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the *same paragraph*. -->